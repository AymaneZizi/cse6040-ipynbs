{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSE 6040, Fall 2015 [22]: Linear regression via least squares, Part 2 -- Error analysis and algorithms\n",
    "\n",
    "Last time we motivated the linear regression modeling problem using some data. The next question is, how do you actually compute the model? Rather than relying on a canned library (which is what we recommend you do in practice), the goal of the previous lesson and this one is to give you a glimpse into the inner-workings of these methods, which bring in a lot of subtle (but hopefully also fun and interesting!) numerical computing issues.\n",
    "\n",
    "To complete today's notebook, you may find these resources helpful:\n",
    "\n",
    "* SciPy solvers documentation, including linear systems solvers: http://docs.scipy.org/doc/scipy/reference/linalg.html\n",
    "* A numerical linear algebra textbook by [Demmel](http://epubs.siam.org/doi/book/10.1137/1.9781611971446)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review: Perturbation theory and round-off error\n",
    "\n",
    "Let's recall a few of the \"big ideas\" from the previous notebook:\n",
    "* How numbers are stored (IEEE-754 format)\n",
    "* How basic operations work -- rounding (or round-off) errors\n",
    "* How errors propagate -- perturbation theory\n",
    "* How to measure \"hardness\" of a numerical problem -- conditioning\n",
    "* How to assess an algorithm -- stability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### IEEE-754 floating-point arithmetic\n",
    "\n",
    "The first idea is to store real values in a standard floating-point format. Most machines implement the [_IEEE-754_ standard](http://grouper.ieee.org/groups/754/). It specifies two main precisions (i.e., formats with a certain number of digits):\n",
    "\n",
    "* _IEEE single-precision_, which is a 32-bit format with an explicit sign bit, 24 bits of mantissa (23 bits stored explicitly, with an implicit leading `1` bit for all values _except_ 0), and an exponent range of $[-126, 127]$. It has a _machine epsilon_ of $\\epsilon_s = 2^{-23} \\approx 1.19 \\times 10^{-7}$.\n",
    "\n",
    "* _IEEE double-precision_, which is a 64-bit format with an explicit sign bit, 53 bits of mantissa (52 bits stored explicitly), and an exponent range of $[-1022, 1023]$. It has a machine epsilon of $\\epsilon_d = 2^{-52} \\approx 2.23 \\times 10^{-308}$.\n",
    "\n",
    "Recall that \"machine epsilon\" refers to the smallest $\\epsilon$ such that $1.0 + \\epsilon$ is representable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rounding (round-off) errors\n",
    "\n",
    "One consequence of a finite-precision representation of numbers is that the results of a sequence of arithmetic operations _may_ depend on the order in which we perform them. For instance, let $\\mathtt{eps} \\equiv \\epsilon$ denote machine epsilon for a given numerical type. Then the following two programs deliver different results, even though mathematically they appear to be equivalent.\n",
    "\n",
    "_Program 1_:\n",
    "\n",
    "    s = 1.0 - eps\n",
    "    t = s + eps # Does 't' equal 1.0?\n",
    "    \n",
    "_Program 2_:\n",
    "\n",
    "    s = 1.0 + eps\n",
    "    t = s - eps # Does 't' equal 1.0?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Program 1 _subtracts_ first. Both the values of `s` and `t` can be represented _exactly_ in the floating-point encoding. By contrast, Program 2 _adds_ first. Since the intermediate result, `s`, _cannot_ be represented exactly, the subsequent value of `t` will differ from that produced by the first program.\n",
    "\n",
    "> Note: The IEEE standard guarantees that given two finite-precision floating-point values, the result of applying any binary operator to them is the same as if the operator were applied in infinite-precision and then rounded back to finite-precision. The precise nature of rounding can be controlled by so-called _rounding modes_; the default rounding mode is \"[round-half-to-even](http://en.wikipedia.org/wiki/Rounding).\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last time, we defined a function called, `print_float_bin()`, which you can use for debugging and printing the raw bit-strings corresponding to a given floating-point value. This function is now a part of the `cse6040utils` module, so grab the latest version thereof to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cse6040utils as cse6040\n",
    "\n",
    "a = -1.0\n",
    "b = 2.**(-52)  # Recall: \\epsilon_d\n",
    "c = b / 2.\n",
    "\n",
    "cse6040.print_float_bin (a, prefix=\"a\")\n",
    "cse6040.print_float_bin (b, prefix=\"b\")\n",
    "cse6040.print_float_bin (c, prefix=\"c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, how do we quantify errors mathematically? Here is a \"simple\" model: every scalar floating-point operation incurs some _bounded_ relative error. Let $a \\odot b$ be the exact result of some mathematical operation on $a$ and $b$, and let $\\mathrm{fl}(a \\odot b)$ be the computed value, after rounding in finite-precision. We will model their relationship by, \n",
    "\n",
    "$$\\mathrm{fl}(a \\odot b) \\equiv (a \\odot b) (1 + \\delta),$$\n",
    "\n",
    "where $|\\delta| \\leq \\epsilon$, machine epsilon.\n",
    "\n",
    "Note: Every operation in an algorithm or program might incur a different value of $\\delta$; when you analyze a computation, you will allow these errors to accumulate individually, and then estimate an _error bound_ by invoking the relationship that $|\\delta| \\leq \\epsilon$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perturbation theory\n",
    "\n",
    "Given a computational problem, will it be \"easy\" or \"hard\" to compute numerically? Given a numerical algorithm, is its behavior \"good\" or \"bad,\" given the intrinsic difficulty of the computational problem? Last time, we discussed how we might assess \"goodness\" analytically, using a mathematical tool from numerical analysis called _perturbation theory_.\n",
    "\n",
    "The basic idea is simple. Given a target mathematical function that we wish to compute, $f(x)$, start by considering a perturbation to its input, $f(x + \\Delta x)$; then, ask how far $f(x + \\Delta x)$ is from $f(x)$. For example, suppose $f(x)$ is continuous and twice differentiable. Then, you might estimate this difference by the usual Taylor series expansion and truncation you learned in Calculus 101, namely,\n",
    "\n",
    "$$f(x + \\Delta x) \\approx f(x) + \\Delta x \\cdot f'(x),$$\n",
    "\n",
    "where $f'(x)$ is the first derivative of $f(x)$ at the point $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Absolute condition numbers -- on the \"hardness\" of a problem\n",
    "\n",
    "From the above relation, you can compute an _upper-bound_ on the absolute value of the error, which would be\n",
    "\n",
    "$$\\left|f(x + \\Delta x) - f(x)\\right| \\approx \\left|\\Delta x\\right| \\cdot \\left|f'(x)\\right|.$$\n",
    "\n",
    "This calculation says that the error depends not only on the measurement error, $\\Delta x$, _but also_ the nature of the function itself at $x$ through the factor, $\\left|f'(x)\\right|$. Indeed, we will give this factor a special name of _absolute condition number_ of evaluating $f$ at $x$. For any given computational problem, we will try to find condition numbers to help us quantify the \"hardness\" of the problem.\n",
    "\n",
    "By way of terminology, if the condition number is small we say the problem of computing $f(x)$ is _well-conditioned_, meaning it should be relatively \"easy\" to compute; otherwise, we say the problem is _ill-conditioned_, meaning it is relatively difficult to compute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Relative condition numbers\n",
    "\n",
    "Besides the absolute value of the difference, you can also ask about the _relative_ difference (error),\n",
    "\n",
    "$$\\left|f(x + \\Delta x) - f(x)\\right| / \\left|f(x)\\right|.$$\n",
    "\n",
    "For this case and problem of evaluating $f(x)$, the relative error becomes,\n",
    "\n",
    "$$\\frac{\\left|f(x + \\Delta x) - f(x)\\right|}\n",
    "      {\\left|f(x)\\right|}\n",
    "\\approx\n",
    " \\frac{|\\Delta x|}\n",
    "      {|x|}\n",
    "   \\cdot\n",
    "\\underbrace{\n",
    " \\frac{\\left|f'(x)\\right| \\cdot |x|}\n",
    "      {\\left|f(x)\\right|}\n",
    "}_{\\equiv\\ \\kappa_r(x)}\n",
    ",$$\n",
    "\n",
    "where $\\kappa_r(x)$ is the _relative condition number_ of evaluating $f(x)$ at $x$.\n",
    "      \n",
    "Observe that this relation expresses the relative change in the output as a function of some relative change in the input, $\\frac{|\\Delta x|}{|x|}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backward stability: on the \"quality\" of an algorithm\n",
    "\n",
    "Let's say someone devises an algorithm to compute $f(x)$. For a given value $x$, let's suppose this algorithm produces the value $\\mathrm{alg}(x)$. Again, we might ask whether $\\mathrm{alg}(x)$ produces a reasonable output.\n",
    "\n",
    "One way to answer this question is to determine if the algorithm is _backward stable_. In particular, a backward stable algorithm is one that computes the exact answer to a slightly different input, i.e.,\n",
    "\n",
    "$$\\mathrm{alg}(x) = f(x + \\Delta x).$$\n",
    "\n",
    "That should look familiar! If an algorithm is backward stable, it means you can estimate its (absolute) backward error using our perturbation analysis from before, i.e.,\n",
    "\n",
    "$$\\left|\\mathrm{alg}(x) - f(x)\\right| \\approx \\left|f'(x)\\right| \\cdot \\left|\\Delta x\\right|,$$\n",
    "\n",
    "or its relative backward error by suitable normalization, i.e.,\n",
    "\n",
    "$$\n",
    "\\frac{\\left|\\mathrm{alg}(x) - f(x)\\right|}\n",
    "     {\\left|f(x)\\right|}\n",
    "\\approx\n",
    "  \\frac{|\\Delta x|}\n",
    "       {|x|}\n",
    "  \\cdot\n",
    "    \\frac{\\left|f'(x)\\right| \\cdot |x|}\n",
    "         {\\left|f(x)\\right|}\n",
    ".\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Computing a sum\n",
    "\n",
    "Let $p$ be a vector (array) of length $n$. Suppose we wish to sum its values. Mathematically, denote the exact sum by,\n",
    "\n",
    "  $$s_{n-1} = \\sum_{i=0}^{n-1} p_i = p_0 + p_1 + p_2 + \\cdots + p_{n-1}.$$\n",
    "  \n",
    "Now consider the following Python program to compute its sum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def alg_sum (p): # p = p[0:n]\n",
    "    s = 0.\n",
    "    for p_i in p:\n",
    "        s += p_i\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to know to what extent this algorithm is \"good.\" We will carry out this analysis by estimating the total round-off error of this algorithm,\n",
    "\n",
    "$$\\mbox{alg_sum}(\\mathtt{p[:]}) - s_{n-1}.$$\n",
    "\n",
    "We will assume that each element of the input array, `p[i]`, is exactly $p_i$, and that the `for` loop enumerates each element `p[i]` in increasing order from `i=0` to `n-1`.\n",
    "\n",
    "The `alg_sum` program performs $n$ additions. Denote the numerical result of each addition by $\\hat{s}_i$, where $i \\in [0, n)$. Since the accumulator variable `s` is initially 0, let's further suppose that the \"zero-th\" addition, which is `0 + p[0]` is performed exactly. In other words, $\\hat{s}_0 = p_0 (1 + \\delta_0) = p_0$, if we take the round-off error $\\delta_0 = 0$.\n",
    "\n",
    "Now consider the $i$-th addition in exact arithmetic, $s_i$, for each $i > 0$. When $\\mbox{alg_sum}$ carries out this addition, it incurs a round-off error, $\\delta_i$. Let's analyze how these errors accumulate.\n",
    "\n",
    "Start by considering the first intermediate result, $\\hat{s}_1$:\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "  \\hat{s}_1 & = & (\\hat{s}_0 + p_1) (1 + \\delta_1) \\\\\n",
    "            & = & \\hat{s}_0 + p_1 + (\\hat{s}_0 + p_1)\\delta_1 \\\\\n",
    "            & = & \\underbrace{p_0 + p_1}_{= s_1} + (p_0 + p_1)(1 + \\delta_1) \\\\\n",
    "      & = & s_1 + s_1\\delta_1.\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "That is, $\\hat{s}_1$ is the exact sum, $s_1$, _plus_ an error proportional to $s_1$.\n",
    "\n",
    "Consider the next computed result, $\\hat{s}_2$:\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "  \\hat{s}_2 & = & (\\hat{s}_1 + p_2) (1 + \\delta_2) \\\\\n",
    "            & = & \\hat{s}_1 + p_2 + (\\hat{s}_1 + p_2)\\delta_2 \\\\\n",
    "            & = & s_1 + s_1\\delta_1 + p_2 + (s_1 + s_1\\delta_1 + p_2)\\delta_2 \\\\\n",
    "            & = & s_2 + s_1\\delta_1 + s_2\\delta_2 + s_1\\delta_1\\delta_2.\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Expanding sums is certainly tedious. Consequently, let's simplify things a bit. The round-off errors _should_ be small values. Thus, for some value $x$, the quantity, $x \\delta_1 \\delta_2$, should be tiny _compared to_ $x \\delta_1$. As such, we can try to approximate $\\hat{s}_2$ by dropping the term containing the $\\delta_1 \\delta_2$ product, i.e.,\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "  \\hat{s}_2 & \\approx & s_2 + s_1\\delta_1 + s_2\\delta_2.\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Let's try one more partial sum, $\\hat{s}_3$, to see if a pattern emerges.\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "  \\hat{s}_3\n",
    "    &    =    & \\hat{s}_2 + p_3 + (\\hat{s}_2 + p_3)(1 + \\delta_3) \\\\\n",
    "    & \\approx & s_2 + s_1\\delta_1 + s_2\\delta_2 + p_3\n",
    "                    + (s_2 + s_1\\delta_1 + s_2\\delta_2 + p_3)\\delta_3 \\\\\n",
    "    & \\approx & s_3 + s_1\\delta_1 + s_2\\delta_2 + s_3\\delta_3.\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Continuing, and recalling that $\\delta_0 = 0$, we would find,\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "  \\hat{s}_{n-1}\n",
    "    & \\approx & s_{n-1} + \\sum_{i=0}^{n-1} s_i \\delta_i \\\\\n",
    "    &    =    & s_{n-1} + \\sum_{i=0}^{n-1} \\left(\\sum_{j=0}^{i} p_i \\right) \\delta_i \\\\\n",
    "    &    =    & s_{n-1} + \\sum_{i=0}^{n-1} i \\cdot p_i \\cdot \\delta_i.\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "(You should verify the expansion and subsequent simplification of the sum.)\n",
    "\n",
    "In other words, the computed sum, $\\hat{s}_{n-1}$, approximately equals the exact sum $s_{n-1}$ plus some accumulated errors.\n",
    "\n",
    "Given this partial result, let's analyze it in two ways: one to show when we should expect the algorithm to be backward stable, and another to derive a bound on the algorithm's relative error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis 1: Backward stability.** Let's rearrange the right-hand side of the above approximation. \n",
    "\n",
    "$$\n",
    "\\hat{s}_{n-1} \\approx\n",
    "  \\left(\\sum_{i=0}^{n-1} p_i\\right)\n",
    "  + \\left(\\sum_{0=1}^{n-1} i \\cdot p_i \\cdot \\delta_i\\right)\n",
    "  = \\sum_{i=0}^{n-1} p_i \\left(1 + i \\cdot \\delta_i \\right)\n",
    ".$$\n",
    "\n",
    "You can see that the computed result, $\\hat{s}_{n-1}$, is nearly the exact solution of a _perturbed_ problem, namely, one in which each input element, $p_i$, is perturbed by $1 + i\\cdot\\delta_i$. Thus, this algorithm will be backward stable if all the $i \\cdot \\delta_i$ are \"small\" relative to 1.\n",
    "\n",
    "This assumption is probably reasonable: in double-precision, $|\\delta_i| \\leq \\epsilon_d = 2^{-52} \\approx 10^{-16}$. So, if you are summing $n$ values, then you can use this algorithm to sum on the order of $n \\sim \\mathcal{O}(1\\mbox{ trillion})$ double-precision values while keeping $n\\epsilon_d \\lesssim 1/1000.$ Thus, the algorithm is backward stable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis 2: A relative error bound.** Our model of floating-point arithmetic says that $|\\delta_i| \\leq \\epsilon$. Thus, you can bound the absolute error as follows:\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "  \\left|\\hat{s}_{n-1} - s_{n-1}\\right|\n",
    "    & \\approx & \\left| \\sum_{i=0}^{n-1} i \\cdot p_i \\cdot \\delta_i \\right| \\\\\n",
    "    &   \\leq  & \\sum_{i=0}^{n-1} i \\cdot \\left|p_i\\right| \\cdot \\left|\\delta_i\\right| \\\\\n",
    "    &   \\leq  & n \\epsilon \\sum_{i=0}^{n-1} \\left|p_i\\right|.\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Treating $p$ as a vector, you can express this error more compactly using the vector 1-norm, i.e.,\n",
    "\n",
    "$$ \\left|\\hat{s}_{n-1} - s_{n-1}\\right| \\leq n \\epsilon \\|p\\|_1 .$$\n",
    "\n",
    "Finally, you can translate this absolute error into a relative one.\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "  \\frac{\\left|\\hat{s}_{n-1} - s_{n-1}\\right|}\n",
    "       {\\left|s_{n-1}\\right|}\n",
    "    & \\lessapprox & n \\epsilon \\frac{\\|p\\|_1}\n",
    "                                    {\\left|s_{n-1}\\right|} \\\\\n",
    "    &      =      & n \\epsilon \\frac{\\sum_{i=0}^{n-1} \\left|p_i\\right|}\n",
    "                                    {\\left| \\sum_{i=0}^{n-1} p_i \\right|}\n",
    "  .\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "While this bound can be small, it can also be much less encouraging than the absolute error bound. (Why?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A numerical experiment.\n",
    "\n",
    "Let's compare the bounds above to an experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n = [10, 100, 1000, 10000, 100000, 1000000, 10000000]\n",
    "x = [0.1] * max (n)\n",
    "s = [1., 10., 100., 1000., 10000., 100000., 1000000.] # exact result\n",
    "t = [alg_sum (x[0:n_i]) for n_i in n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rel_err_computed = [abs (ti-si) / abs (si) for (si, ti) in zip (s, t)]\n",
    "rel_err_bound = [float (ni) * (2.**(-52)) for ni in n]\n",
    "\n",
    "# Plot of the relative error bound\n",
    "plt.loglog (n, rel_err_computed, 'b*', n, rel_err_bound, 'r--')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Computing dot products\n",
    "\n",
    "Let $x$ and $y$ be two vectors of length $n$, and denote their dot product by $f(x, y) \\equiv x^T y$.\n",
    "\n",
    "Now suppose we store the values of $x$ and $y$ _exactly_ in two Python arrays, `x[0:n]` and `y[0:n]`. Further suppose we compute their dot product by the program, `alg_dot()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def alg_dot (x, y):\n",
    "    p = [xi*yi for (xi, yi) in zip (x, y)]\n",
    "    s = alg_sum (p)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Derive absolute and relative error bounds for `alg_dot()`. Is `alg_dot()` backward stable?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> _Answer_: Let $\\hat{p}_i$ denote the error in the $i$-th product, i.e.,\n",
    ">\n",
    ">   $$\\hat{p}_i = x_i y_i \\left(1 + \\gamma_i\\right).$$\n",
    ">\n",
    "> We then invoke `alg_sum()` to sum these rounded products, producing a computed dot product, $\\hat{s}_{n-1}$. Therefore, you can invoke the corresponding error bound, on these products:\n",
    ">\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "  \\hat{s}_{n-1}\n",
    "    & \\approx & \\sum_i \\hat{p}_i\n",
    "                         \\left(1 + i\\cdot \\delta_i\\right) \\\\\n",
    "    &    =    & \\sum_i x_i y_i\n",
    "                         \\left(1 + \\gamma_i\\right)\n",
    "                         \\left(1 + i\\cdot \\delta_i\\right) \\\\\n",
    "    &    =    & \\sum_i x_i y_i\n",
    "                + \\sum_i x_i y_i \\left(\\gamma_i + i \\cdot \\delta_i\\right)\n",
    "                + \\sum_i x_i y_i i \\cdot \\gamma_i \\delta_i \\\\\n",
    "    & \\approx & \\left( \\sum_i x_i y_i \\right)\n",
    "                + \\sum_i x_i y_i (\\gamma_i + i \\cdot \\delta_i),\n",
    "\\end{array}\n",
    "$$\n",
    ">\n",
    "> where we have invoked an argument that the $i \\gamma_i \\delta_i$ terms are small compared to 1. This computation shows what the absolute error in the computed result is, namely, the absolute value of the last summation:\n",
    ">\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "  \\left| \\hat{s}_{n-1} - \\sum_i x_i y_i \\right|\n",
    "    & \\approx & \\left| \\sum_i x_i y_i (\\gamma_i + i \\cdot \\delta_i) \\right| \\\\\n",
    "    &   \\leq  & \\sum_i |x_i| \\cdot |y_i|\n",
    "                       \\cdot \\left| \\gamma_i + i \\cdot \\delta_i \\right| \\\\\n",
    "    &   \\leq  & (n+1) \\epsilon \\cdot |x|^T |y|,\n",
    "\\end{array}\n",
    "$$\n",
    ">\n",
    "> where $|x|$ and $|y|$ refer to componentwise absolute values.\n",
    ">\n",
    "> **Relative error bounds.** Just divide both sides by the absolute value of the exact dot product:\n",
    ">\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "  \\frac{\\left| \\hat{s}_{n-1} - x^T y \\right|}\n",
    "       {\\left| x^T y \\right|}\n",
    "    & \\approx & (n+1) \\epsilon \\frac{|x|^T |y|}{|x^T y|}.\n",
    "\\end{array}\n",
    "$$\n",
    ">\n",
    "> **Backward stability.** This result shows that, with respect to absolute error, `alg_dot()` will be backward stable. Define a vector $\\tilde{y} \\equiv \\left( \\tilde{y}_i \\right)$, where each component is\n",
    ">\n",
    "$$\\tilde{y}_i \\equiv y_i \\left(1 + \\gamma_i + i \\cdot \\delta_i\\right).$$\n",
    ">\n",
    "> Provided $\\gamma_i + i \\cdot \\delta_i$ are small compared to 1, $\\tilde{y}$ will be a small perturbation to the input vector, $y$, and therefore `alg_dot()` will compute the exact solution to a slightly different problem, $x^T \\tilde{y}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving overdetermined linear systems via least squares\n",
    "\n",
    "Having taken that detour, we can now return to our original motivating problem: solving an overdetermined system of linear equations, $Ax=b$ where the real-valued $m \\times n$ matrix $A \\in \\mathbb{R}^{m \\times n}$ has at least as many rows as columns ($m \\geq n$). Let's further assume that $A$ has full rank ($\\mathrm{rank}(A) = n$), i.e., the columns of $A$ are linearly independent.\n",
    "\n",
    "Since the system is overdetermined, it will not have a unique solution. Therefore, we will need to compute a \"best fit\" approximate solution. We will look at a couple different algorithms for solving this system. Then, using the analysis techniques mentioned above, see how we might determine what method we should use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need several facts from linear algebra, some of which appear as exercises, like this one:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise.** Let $x \\in \\mathbb{R}^n$ be a real-valued vector of length $n$. Show that $\\|x\\|_2^2 = x^T x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> _@YOUSE: Enter your answer(s) here._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that the vector two-norm, $\\|\\cdot\\|_2$, gives you a way to measure the \"length\" of a vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise.** Given two vectors, $x$ and $y$, show that the dot product is commutative, i.e., $x^T y = y^T x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> _Answer_: $x^T y = \\sum_i x_i y_i = \\sum_i y_i x_i = y^T x.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perturbation theory for linear systems\n",
    "\n",
    "Let's start by asking how \"hard\" it is to solve a given linear system, $Ax=b$. You will apply perturbation theory to answer this question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, some notation. To measure the magnitudes of the perturbations, we will use vector and matrix norms. Assume that the norm of a vector, $\\|x\\|_2$, denotes the vector 2-norm; further assume that the norm of a matrix, $\\|A\\|_F$, denotes the matrix Frobenius norm. If you need a refesher on these definitions, see our [linear algebra notes](https://t-square.gatech.edu/access/content/group/gtc-7308-4387-56aa-b79e-a3f4c812167d/Kuang-2014-linalg-notes.pdf). The most important identities for the discussion below are:\n",
    "\n",
    "* _Triangle inequality_: $\\|x + y\\|_2 \\leq \\|x\\|_2 + \\|y\\|_2$\n",
    "* _Norm of a matrix-vector product_: $\\|Ax\\|_2 \\leq \\|A\\|_F\\cdot\\|x\\|_2$\n",
    "* _Norm of matrix-matrix product_: $\\|AB\\|_F \\leq \\|A\\|_F\\cdot\\|B\\|_F$\n",
    "\n",
    "To simplify the notation a little, we will drop the \"$2$\" and \"$F$\" subscripts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, suppose all of $A$, $b$, and the eventual solution $x$ undergo additive perturbations, denoted by $A + \\Delta A$, $b + \\Delta b$, and $x + \\Delta x$, respectively. Then, subtracting the original system from the perturbed system, you would obtain the following.\n",
    "\n",
    "$$\n",
    "\\begin{array}{rrcll}\n",
    "   &         (A + \\Delta A)(x + \\Delta x) & = & b + \\Delta b & \\\\\n",
    "- [&                                   Ax & = & b & ] \\\\\n",
    "\\hline\n",
    "   & \\Delta A x + (A + \\Delta A) \\Delta x & = & \\Delta b & \\\\\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now look more closely at the perturbation, $\\Delta x$, of the solution. Let $\\hat{x} \\equiv x + \\Delta x$ be the perturbed solution. Then the above can be rewritten as,\n",
    "\n",
    "$$\\Delta x = A^{-1} \\left(\\Delta b - \\Delta A \\hat{x}\\right),$$\n",
    "\n",
    "where we have assumed that $A$ is invertible. (That won't be true for our overdetermined system, but let's not worry about that for the moment.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How large is $\\Delta x$? Let's use a norm to measure it and bound it using \n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "  \\|\\Delta x\\| &   =   & \\|A^{-1} \\left(\\Delta b - \\Delta A \\hat{x}\\right)\\| \\\\\n",
    "               &  \\leq & \\|A^{-1}\\|\\cdot\\left(\\|\\Delta b\\| + \\|\\Delta A\\|\\cdot\\|\\hat{x}\\|\\right).\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can rewrite this as follows:\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "  \\frac{\\|\\Delta x\\|}\n",
    "       {\\|\\hat{x}\\|}\n",
    "    & \\leq &\n",
    "    \\|A^{-1}\\| \\cdot \\|A\\| \\cdot \\left(\n",
    "                                   \\frac{\\|\\Delta A\\|}\n",
    "                                        {\\|A\\|}\n",
    "                                   +\n",
    "                                   \\frac{\\Delta b}\n",
    "                                        {\\|A\\| \\cdot \\|\\hat{x}\\|}\n",
    "                                 \\right).\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This bound says that the relative error of the perturbed solution, compared to relative perturbations in $A$ and $b$, scales with the product, $\\|A^{-1}\\| \\cdot \\|A\\|$. This factor is the linear systems analogue of the condition number for evaluating the function $f(x)$! As such, we define\n",
    "\n",
    "$$\\kappa(A) \\equiv \\|A^{-1}\\| \\cdot \\|A\\|$$\n",
    "\n",
    "as the _condition number of $A$_ for solving linear systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A numerical example of an ill-conditioned system\n",
    "\n",
    "Let's look at a system that is ill-conditioned and see what happens when we make a tiny perturbation to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "A = np.array([(1., 1000.),\n",
    "              (2.**(-10) + 2.**(-11), 1.)])\n",
    "\n",
    "print \"A ==\\n\", A\n",
    "print \"\\ncond (A) == \", np.linalg.cond (A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Delta_A = np.array ([(0., 0.),\n",
    "                     (-2.**(-11), 0.)\n",
    "                    ])\n",
    "B = A + Delta_A\n",
    "\n",
    "print \"B := A + dA ==\\n\", B\n",
    "print \"\\ncond (B) / cond (A) == \", \\\n",
    "      np.linalg.cond (B) / np.linalg.cond (A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b = np.array([1., 1.])\n",
    "\n",
    "x_A = np.linalg.solve (A, b)\n",
    "print \"x ~= A^(-1)*b == \", x_A\n",
    "\n",
    "x_B = np.linalg.solve (B, b)\n",
    "print \"x ~= B^(-1)*b == \", x_B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Least squares minimization\n",
    "\n",
    "If $Ax=b$ is overdetermined, then there are more equations (rows of $Ax$) than unknowns (entries of $x$) and no solution in general. Therefore, we ask for an approximate solution $x$. How do we choose $x$?\n",
    "\n",
    "One intuitive idea is to choose an $x$ such that the _residual_, $r = r(x) \\equiv b - Ax$, is minimized in some way, such as measuring the length of $r$ using the vector two-norm:\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "  \\arg\\!\\min_{x} \\|r(x)\\|_2^2\n",
    "    & = & \\arg\\!\\min_{x} \\|b - Ax\\|_2^2 \\\\\n",
    "    & = & \\arg\\!\\min_{x} (b - Ax)^T(b - Ax) \\\\\n",
    "    & = & \\arg\\!\\min_{x} \\left\\{ b^T b - 2 x^T A^T b + x^T A^T A x \\right\\}.\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradients.** To find the minimum $x$, we need to do the moral equivalent of taking a \"vector derivative,\" setting it to 0, and then solving for $x$. The right mathematical tool is the _gradient_. Given a _scalar_ function $f(x)$, where $x$ is a vector, the function's gradient, $\\nabla_x f(x)$, is a _vector_ whose $k$-th entry is the partial derivative of $f(x)$ with respect to $x_k$. That is,\n",
    "\n",
    "$$\n",
    "\\nabla_x f(x) \\equiv\n",
    "  \\left(\\begin{array}{c}\n",
    "    \\frac{\\partial f}{\\partial x_0} \\\\\n",
    "    \\frac{\\partial f}{\\partial x_1} \\\\\n",
    "    \\vdots \\\\\n",
    "    \\frac{\\partial f}{\\partial x_{n-1}}\n",
    "  \\end{array}\\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise.** Let $x$ and $y$ be vectors, and let $M$ be a matrix. Verify the following identities related to the gradient.\n",
    "\n",
    "1. $\\nabla_x (x^T y) = y$\n",
    "2. $\\nabla_x (x^T x) = 2x$\n",
    "3. $\\nabla_x (x^T M x) = (M + M^T)x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> _@YOUSE_: Enter your answer(s) here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise.** Let $f(x) \\equiv (b - Ax)^T(b - Ax)$, where $x \\in \\mathbb{R}^n$, $b \\in \\mathbb{R}^m$, and $A \\in \\mathbb{R}^{m \\times n}$. Show that\n",
    "\n",
    "$$\\nabla_x f(x) = 2 (A^T A x - A^T b).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> _@YOUSE_: Enter your answer(s) here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Normal equations.** In the previous exercise, $f(x)$ is precisely the objective function we wish to minimize by a suitable choice of $x$. The minimum occurs when $\\nabla_x f(x) = 0$; per the execises above, this $x$ is the solution to the _normal equations_,\n",
    "\n",
    "$$A^T A x = A^T b.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise.** You could solve this system directly, by first forming $C \\leftarrow A^TA$ and $c \\leftarrow A^T b$, and then solving $Cx=c$. But is this a good algorithm? (You may assume $C$ is invertible.) Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> _@YOUSE_: Enter your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QR factorization.** In fact, the standard method to solve the linear least squares problem is to use a so-called QR factorization.\n",
    "\n",
    "As it happens, every full-rank matrix $A \\in \\mathbb{R}^{m \\times n}$, with $m \\geq n$, may be written as the product $A = QR$, where $Q$ is an $m \\times n$ orthogonal matrix (i.e., $Q^T Q = I$) and $R$ is upper-triangular with positive diagonals ($r_{ii} > 0$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise.** Suppose you are given a QR decomposition, $A = QR$. Show how to compute the solution $x$ of the linear least squares problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> _@YOUSE_: Enter your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solving the normal equations is generally cheaper than performing a QR factorization. However, using QR is more numerically accurate. Algorithms to compute a QR factorization are numerically stable, and solving the system using QR has a lower condition number than solving by using the normal equations. Additionally, applying (multiplying by) an orthogonal matrix is a stable operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This file implements the experiment in Lecture 19 of\n",
    "# Trefethen and Bau, Numerical Linear Algebra, SIAM 1997.\n",
    "#\n",
    "# Python implementation originally by Da Kuang (2014)\n",
    "\n",
    "import numpy as np\n",
    "import scipy.linalg as lin\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "m = 100\n",
    "n = 15\n",
    "A = np.zeros ((m, n))\n",
    "\n",
    "a = np.arange (0, m, dtype=np.float64)\n",
    "a /= (m-1)\n",
    "\n",
    "for i in range (n):\n",
    "    A[:, i] = np.power (a, i)\n",
    "    \n",
    "print 'Condition number of A:', np.linalg.cond (A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b = np.exp (np.sin (4*a)) # exp (sin (4*a))\n",
    "b /= 2006.787453080206\n",
    "\n",
    "plt.plot (a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result = np.linalg.lstsq (A, b)\n",
    "x1 = result[0]\n",
    "print 'Last element of x1 (possibly from SVD):', x1[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Q, R = np.linalg.qr (A)\n",
    "tmp = Q.T.dot (b)\n",
    "x2 = np.linalg.solve (R, tmp)\n",
    "print 'Last element of x2 (from QR):', x2[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ATA = A.T.dot (A)\n",
    "tmp = A.T.dot (b)\n",
    "x3 = np.linalg.solve (ATA, tmp)\n",
    "print 'Last element of x3 (from normal equation):', x3[-1]\n",
    "\n",
    "try:\n",
    "    L = np.linalg.cholesky (ATA)\n",
    "except np.linalg.linalg.LinAlgError, e:\n",
    "    print 'Cholesky factorization error:', e\n",
    "\n",
    "print 'Condition number of A\\'A:', np.linalg.cond (ATA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: Nearly collinear predictors\n",
    "\n",
    "One practical situation in which poor conditioning can arise in linear regression modeling is when you include two strongly correlated predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tmp = np.arange (100)\n",
    "A = np.zeros ((100, 2))\n",
    "\n",
    "delta_0 = np.random.rand (100) * 0.0001\n",
    "delta_1 = np.random.rand (100) * 0.0001\n",
    "\n",
    "A[:, 0] = tmp + delta_0\n",
    "A[:, 1] = tmp + delta_1\n",
    "\n",
    "ATA = A.T.dot(A)\n",
    "\n",
    "print 'cond (A):', np.linalg.cond (A)\n",
    "print 'cond (A^T*A):', np.linalg.cond (ATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
