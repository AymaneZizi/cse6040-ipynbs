{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSE 6040, Fall 2015 [21]: Linear regression via least squares, Part 1 -- Error analysis\n",
    "\n",
    "Yay! Time for a new topic: _linear regression by the method of least squares_.\n",
    "\n",
    "We will do this topic in two parts: one which is generically about the errors that can arise in carrying out a numerical computation, and the other which is specifically about the linear regression problem.\n",
    "\n",
    "> This lab kicks off the last part of our course, which is a survey of data analysis and mining methods. We will assume you will learn the theory behind these methods in more detail in other classes; our focus will be on algorithmic and implementation concepts.\n",
    "\n",
    "For today's topic, let's use the following dataset, which is a crimes dataset from 1960: http://cse6040.gatech.edu/fa15/uscrime.csv\n",
    "\n",
    "This dataset comes from: http://www.statsci.org/data/general/uscrime.html\n",
    "\n",
    "Other useful references and downloads for today's class:\n",
    "\n",
    "* Please get the latest version of [`cse6040utils.py`](https://raw.githubusercontent.com/rvuduc/cse6040-ipynbs/master/cse6040utils.py)\n",
    "* Python's documentation on its `float` type: https://docs.python.org/2/tutorial/floatingpoint.html\n",
    "* Solvers, including linear systems solvers, in SciPy: http://docs.scipy.org/doc/scipy/reference/linalg.html\n",
    "\n",
    "Also, much of the discussion of round-off error is taken from an [excellent book](http://epubs.siam.org/doi/book/10.1137/1.9781611971446) on numerical linear algebra.\n",
    "\n",
    "> You may be able to use the Georgia Tech library's web proxy service to download electronic chapters of this book. Also, this book was written by Rich's former PhD advisor, so a referral thereto is probably not completely objective. `:)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "\n",
    "Let's start by loading a dataset and motivating the problem of modeling it using linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import cse6040utils as cse6040"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv ('uscrime.csv', skiprows=1)\n",
    "display (df.head ())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row of this dataset is a US State. The columns are described here: http://www.statsci.org/data/general/uscrime.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a quick peek at the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.describe ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Look at a few relationships\n",
    "sns.pairplot (df[['Crime', 'Wealth', 'Ed', 'U1']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we wish to build a model of some quantity, called the _response_ variable, given some set of _predictors_. In the US crimes dataset, the response might be the crime rate (`Crime`), which we wish to predict from the predictors of income (`Wealth`), education (`Ed`), and the unemployment rate of young males (`U1`).\n",
    "\n",
    "In a linear regression model, we posit that the response is a linear function of the predictors. That is, suppose there are $m$ observations in total and consider the $i$-th observation. Let $b_i$ be the response of that observation. Then denote the $n$ predictors for observation $i$ as $\\{a_{i,1}, a_{i,2}, \\ldots, a_{i,n}\\}$. From this starting point, we might then posit a _linear_ model of $b$ having the form,\n",
    "\n",
    "$b_i = x_0 + a_{i,1} x_1 + a_{i,2} x_2 + \\cdots + a_{i,n} x_n$,\n",
    "\n",
    "where we wish to compute the \"best\" set of coefficients, $\\{x_0, x_1, \\ldots, x_n\\}$. Note that this model includes a constant offset term, $x_0$. Since we want this model to hold for observations, then we effectively want to solve the system of $m$ equations in $n+1$ unknowns,\n",
    "\n",
    "$\\left(\n",
    "  \\begin{array}{c}\n",
    "  b_1 \\\\\n",
    "  b_2 \\\\\n",
    "  \\vdots \\\\\n",
    "  b_m\n",
    "  \\end{array}\n",
    " \\right)\n",
    "$\n",
    "=\n",
    "$\\left(\n",
    "   \\begin{array}{ccccc}\n",
    "     1. & a_{1,1} & a_{1,2} & \\ldots & a_{1,n} \\\\\n",
    "     1. & a_{2,1} & a_{2,2} & \\ldots & a_{2,n} \\\\\n",
    "        &         & \\cdots  &        &         \\\\\n",
    "     1. & a_{m,1} & a_{m,2} & \\ldots & a_{m,n}\n",
    "   \\end{array}\n",
    " \\right)\n",
    " \\left(\\begin{array}{c}\n",
    "   x_0 \\\\\n",
    "   x_1 \\\\\n",
    "   \\vdots \\\\\n",
    "   x_n\n",
    " \\end{array}\\right),\n",
    "$\n",
    " \n",
    "or just $Ax=b$. Typically, there are many more observations than parameters ($m \\gg n$), in which case we say this linear system is _overdetermined_.\n",
    "\n",
    "So how do we compute $x$? Most statistical software and libraries hide the details of computing $x$ from you. However, it's important to understand at least a little bit about what happens under the hood, which is today's topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Aside: Models, Errors, and Algorithms -- Oh my!\n",
    "\n",
    "The main task of data analysis is to help explain and understand some phenomenon by building a mathematical model from data, and analyzing the model or using it to make predictions. During this process, there are several potential sources of error, including:\n",
    "\n",
    "* _measurement errors_ in the data, due to limitations in our observational equipment or methodology;\n",
    "* _modeling error_, due to our mathematical model having simplifying assumptions;\n",
    "* _truncation error_, due to limitations in our computer algorithms; and\n",
    "* _rounding error_, due to the fact that we must represent all values on the computer in _finite precision_.\n",
    "\n",
    "In this course, we will mostly leave measurement and modeling errors as a topics for other courses. Instead, we will focus on truncation and rounding errors, and to a lesser extent, assessing the potential impact of measurement error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise.** Are there other kinds of errors not captured in the list above?\n",
    " \n",
    "**Exercise.** Give examples of the kinds of errors that might arise in our motivating problem (i.e., building a linear regression model of crime rates)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_@YOUSE: Enter your discussion / solutions here_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Floating-point arithmetic\n",
    "\n",
    "Real values are typically stored in _IEEE floating-point format_. If you have programmed in other languages, you may have seen scalar data types for _single-precision_ and _double-precision_ formats (e.g., `float` and `double` in C/C++/Java). A \"floating-point\" encoding is basically a normalized scientific notation consisting of a _base_, a _sign_, a fractional _mantissa_, and an integer _exponent_. Let's look at an example to see how this might work.\n",
    "\n",
    "Consider the value 0.125. In a normalized scientific notation, we would write this number as $+1.25 \\times 10^{-2}$, where the base is 10, the mantissa is 1.25, and the exponent is -2. Conceptually, if we always used base 10 for all our floating-point values, then our floating-point encoding of this value would, conceptually, be a tuple $(+, 1.25, -2)$. \n",
    "\n",
    "However, we cannot store an infinite number of digits for the mantissa and exponent values. Thus, we would normally _also_ limit the number of digits that may appear in either. We might use, say, 6 digits for the mantissa and 2 digits (ignoring the sign) for the exponent, i.e., a tuple of the form $(\\pm, m.mmmmm, \\pm xx)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise.** What is the largest value we could represent in this format? What is the smallest value? What is the smallest _positive_ value we could represent? How would we encode these values?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_@YOUSE: Enter your solutions here_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise.** Encode the following values as tuples:\n",
    "\n",
    "1. $1.0$\n",
    "2. $-10^{-6}$\n",
    "3. $1.0 - 10^{-6}$\n",
    "4. $1.0 + 10^{-6}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_@YOUSE: Enter your solutions here_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A small surprise? The consequences of finite-precision\n",
    "\n",
    "Let $a=1.0$ and $b=10^{-6}$. Now consider two programs.\n",
    "\n",
    "_Program 1_:\n",
    "\n",
    "    s = a - b\n",
    "    t = s + b\n",
    "    \n",
    "_Program 2_:\n",
    "\n",
    "    s = a + b\n",
    "    t = s - b\n",
    "    \n",
    "If the _precision_, or number of digits in the encoding, were infinite, then both programs would produce `t == a == 1.0`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise.** Suppose we instead use a _finite-precision_ floating-point encoding, using base 10 digits with 6 digits of precision for the mantissa and 2 digits for the exponent, plus separate sign \"digits\" for each. What is the final value of `t` in each of these two programs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_@YOUSE: Enter your solutions here_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preceding examples assume the digits are represented in base 10. However, computers encode all values using _bits_, which are _base 2_ digits. All the same ideas as above apply, but on base 2 values rather than base 10 values.\n",
    "\n",
    "One consequence of this difference is that certain finite-precision decimal fractions _cannot_ be represented exactly!\n",
    "\n",
    "> Can you see why? Consider the decimal value 0.1 represented in a binary format.\n",
    "\n",
    "In addition, the IEEE floating-point standard defines the encoding a little differently than we've used it. First, if the value is not 0, then the mantissa _always_ has an implicit \"1\" as the leading digit; therefore, it needn't be stored explicitly, thereby saving a bit and effectively increasing the precision a little. Secondly, the range of the exponent is not symmetric. In our hypothetical base-10 \"6 + 2\" encoding, we assumed the exponent would range from -99 to 99, which is a symmetric interval; in IEEE floating-point, there will be a slight asymmetry in this range. Part of the reason is that the IEEE floating-point encoding can also represent several kinds of special values, such as infinities and an odd bird called \"not-a-number\" or `NaN`. This latter value, which you may have seen if you have used any standard statistical packages, can be used to encode certain kinds of floating-point exceptions that result when, for instance, you try to divide by zero.\n",
    "\n",
    "In IEEE floating-point, there are two main encodings, known as _single-precision_ (`float` in C/C++/Java) and _double-precision_ (`double` in C/C++/Java). In brief, these differ as follows:\n",
    "\n",
    "* Single-precision: 32 bits total, with 24 bits for the mantissa and an exponent range of [-126, 127].\n",
    "\n",
    "* Double-precision: 64 bits total, with 53 bits for the mantissa and an exponent range of [-1022, 1023]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise.** What is the smallest positive value that can be represented in IEEE single-precision? What about in double-precision?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_@YOUSE: Enter your solutions here_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise.** Consider the smallest possible value greater than 1.0 that can be represented in floating-point. Let's call this value, $1.0 + \\epsilon$.\n",
    "\n",
    "Determine $\\epsilon_s$ and $\\epsilon_d$, the corresponding values of $\\epsilon$ in single-precision and double-precision, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_@YOUSE: Enter your solutions here_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another important consequence of the binary format is that when you print things in base ten, what you see may not be what you get! For instance, try running the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from decimal import Decimal\n",
    "\n",
    "x = 1.0 + 2.0**(-52)\n",
    "\n",
    "print x\n",
    "print Decimal (x) # What does this do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Aside: If you ever need true decimal storage with no loss of precision, turn to the `Decimal` package. Just be warned it will come at a price:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a_native = 1.0\n",
    "b_native = 2.0\n",
    "\n",
    "a_decimal = Decimal ('1.0')\n",
    "b_decimal = Decimal ('2.0')\n",
    "\n",
    "%timeit a_native + b_native\n",
    "%timeit a_decimal + b_decimal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For today's lesson, it will be helpful to occasionally peek at floating-point values \"in the raw.\" For this purpose, we've provided you with a handy routine called `float_to_bin(x)`, which given a floating-point value `x` will return its IEEE representation as a binary string. (It is defined in the `cse6040utils` module.)\n",
    "\n",
    "The following code uses `float_to_bin()` to define another function that dumps a floating-point number's complete `Decimal` form along with its binary form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_float_bin (x, prefix=\"\"):\n",
    "    print (\"%s: %s\\n%s  %s\" % (prefix,\n",
    "                                Decimal (x),\n",
    "                                ' ' * len (prefix),\n",
    "                                cse6040.float_to_bin (x)))    \n",
    "\n",
    "a = -1.0\n",
    "b = 2.**(-52)  # Recall: \\epsilon_d\n",
    "c = b / 2.\n",
    "\n",
    "print_float_bin (a, prefix=\"a\")\n",
    "print_float_bin (b, prefix=\"b\")\n",
    "print_float_bin (c, prefix=\"c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise.** Recall the two program fragments from above:\n",
    "\n",
    "_Program 1_:\n",
    "\n",
    "    s = a - b\n",
    "    t = s + b\n",
    "    \n",
    "_Program 2_:\n",
    "\n",
    "    s = a + b\n",
    "    t = s - b\n",
    "\n",
    "Let $a=1.0$ and $b=\\epsilon_d / 2 \\approx 1.11 \\times 10^{-16}$. Write some Python code to evaluate these two programs and compare their outputs. (To look closely at their outputs, use the `print_float_bin()` function from above.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = 1.0\n",
    "b = ... # What value goes here?\n",
    "\n",
    "s1 =  a - b\n",
    "t1 = s1 + b\n",
    "\n",
    "s2 =  a + b\n",
    "t2 = s2 - b\n",
    "\n",
    "print_float_bin (s1, prefix=\"s1\")\n",
    "print_float_bin (t1, prefix=\"t1\")\n",
    "print (\"\\n\")\n",
    "print_float_bin (s2, prefix=\"s2\")\n",
    "print_float_bin (t2, prefix=\"t2\")\n",
    "\n",
    "print (\"\\n(t1 == t2) == %s\" % (t1 == t2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the way, in Numpy you can determine machine epsilon in a more portable way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "EPS_S = np.finfo (np.float32).eps\n",
    "EPS_D = np.finfo (float).eps\n",
    "\n",
    "print_float_bin (float (EPS_S), prefix=\"eps_s\")\n",
    "print_float_bin (float (EPS_D), prefix=\"eps_d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perturbation theory and condition numbers\n",
    "\n",
    "Given the various sources of error, how do we know whether a given algorithm is \"good\" for computing the solution to a given problem? An important tool in the area of _numerical analysis_ is _perturbation theory_.\n",
    "\n",
    "To see perturbation theory in action, suppose we wish to determine by how much a \"small\" measurement error, $\\Delta x$, affects the output of some function, $f(x)$. Barring any other information and assuming $f(x)$ is continuous and differentiable, we could try to estimate the error of evaluating $f(x + \\Delta x)$ compared to $f(x)$ by the following linear approximation, which comes from a Taylor series expansion:\n",
    "\n",
    "$$f(x + \\Delta x) \\approx f(x) + \\Delta x \\cdot f'(x),$$\n",
    "\n",
    "where $f'(x)$ is the first derivative of $f(x)$ at the point $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Absolute condition numbers.** From this relation, we can compute an upper-bound on the absolute value of the error:\n",
    "\n",
    "$$\\left|f(x + \\Delta x) - f(x)\\right| \\approx \\left|\\Delta x\\right| \\cdot \\left|f'(x)\\right|.$$\n",
    "\n",
    "This calculation says that the error depends not only on the measurement error, $\\Delta x$, _but also_ the nature of the function itself at $x$ through the factor, $\\left|f'(x)\\right|$. Indeed, we will give this factor a special name of _absolute condition number_ of evaluating $f$ at $x$. For any given computational problem, we will try to find condition numbers to help us quantify the \"hardness\" of the problem.\n",
    "\n",
    "That is, for the problem of evaluating $f(x)$, the preceding analysis says that if this factor is not too large, then small measurement errors will lead to only small errors in the output. In this case, we say the problem is _well-conditioned_. If instead this factor is very large, then even very small errors will lead to large errors in the output. In this case, we say the problem is _ill-conditioned_.\n",
    "\n",
    "Put differently, the problem of evaluating $f(x)$ when the condition number is large is inherently _more difficult_ than doing so when the condition number is small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Relative condition numbers.** The error considered above is the absolute error, in contrast to the _relative error_,\n",
    "\n",
    "$$\\left|f(x + \\Delta x) - f(x)\\right| / \\left|f(x)\\right|.$$\n",
    "\n",
    "For this case and problem of evaluating $f(x)$, let's rewrite this slightly as,\n",
    "\n",
    "$$\\frac{\\left|f(x + \\Delta x) - f(x)\\right|}\n",
    "      {\\left|f(x)\\right|}\n",
    "\\approx\n",
    " \\frac{|\\Delta x|}\n",
    "      {|x|}\n",
    "   \\cdot\n",
    "\\underbrace{\n",
    " \\frac{\\left|f'(x)\\right| \\cdot |x|}\n",
    "      {\\left|f(x)\\right|}\n",
    "}_{\\equiv\\ \\kappa_r(x)}\n",
    ",$$\n",
    "\n",
    "where $\\kappa_r(x)$ is the _relative condition number_ of evaluating $f(x)$ at $x$.\n",
    "      \n",
    "Observe that this relation expresses the relative change in the output as a function of some relative change in the input ($|\\Delta x| / |x|$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Backward stability.** Let's say someone devises an algorithm to compute $f(x)$. For a given value $x$, let's suppose this algorithm produces the value $\\mathrm{alg}(x)$. One important question might be, is that output \"good\" or \"bad?\"\n",
    "\n",
    "One property to measure \"goodness\" will be _backward stability_. In particular, we will say that $\\mathrm{alg}(x)$ is a _backward stable algorithm_ to compute $f(x)$ if, for all $x$, there exists a \"small\" $\\Delta x$ such that\n",
    "\n",
    "$$\\mathrm{alg}(x) = f(x + \\Delta x).$$\n",
    "\n",
    "That should look familiar! It means we can estimate the (absolute) backward error using our perturbation analysis from before, i.e.,\n",
    "\n",
    "$$\\left|\\mathrm{alg}(x) - f(x)\\right| \\approx \\left|f'(x)\\right| \\cdot \\left|\\Delta x\\right|.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Round-off errors.** We already know that numerical values can only be represented finitely, which introduces round-off error. Thus, at the very least we should hope that a scheme to compute $f(x)$ is as insensitive to round-off errors as possible.\n",
    "\n",
    "To quantify sensitivity, the standard technique is to assume that every scalar floating-point operation incurs some bounded error. Let $a \\odot b$ be the exact result of some mathematical operation on $a$ and $b$, and let $\\mathrm{fl}(a \\odot b)$ be the computed value, after rounding in finite-precision. We will model the difference by,\n",
    "\n",
    "$$\\mathrm{fl}(a \\odot b) \\equiv (a \\odot b) (1 + \\delta),$$\n",
    "\n",
    "where $|\\delta| \\leq \\epsilon$, machine epsilon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 1: Computing a sum.** Let $p$ be a vector (array) of length $n$. Suppose we wish to sum its values. Mathematically, denote the exact sum by,\n",
    "\n",
    "  $$s_{n-1} = \\sum_{i=0}^{n-1} p_i = p_0 + p_1 + p_2 + \\cdots + p_{n-1}.$$\n",
    "  \n",
    "Now consider the following Python program to compute its sum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def alg_sum (p): # p = p[0:n]\n",
    "    s = 0.\n",
    "    for p_i in p:\n",
    "        s += p_i\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to know to what extent this algorithm is \"good.\" We will carry out this analysis by estimating the total round-off error of this algorithm, $\\mbox{alg_sum}(p_0, \\ldots, p_{n-1}) - s_{n-1}$. We will assume that each element of the input array, `p[i]`, is exactly $p_i$, and that the `for` loop enumerates each element `p[i]` in increasing order from `i=0` to `n-1`.\n",
    "\n",
    "The `alg_sum` program performs $n$ additions. Denote the result of each addition by $t_i$, where $i \\in [0, n)$. Since the accumulator variable is initially 0, let's further suppose that the \"zero-th\" addition, which is `0 + p[0]` is performed exactly; in other words, $t_0 = p_0$.\n",
    "\n",
    "Now consider the $i$-th addition, $s_i$, for each $i > 0$. The addition incurs a round-off error, $\\delta_i$. Let's analyze how these errors accumulate.\n",
    "\n",
    "Start by considering $t_1$:\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "  t_1 & = & (t_0 + p_1) (1 + \\delta_1) \\\\\n",
    "      & = & t_0 + p_1 + (t_0 + p_1)\\delta_1 \\\\\n",
    "      & = & \\underbrace{p_0 + p_1}_{= s_1} + (p_0 + p_1)(1 + \\delta_1) \\\\\n",
    "      & = & s_1 + s_1\\delta_1.\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "That is, $t_1$ is the exact sum, $s_1$, _plus_ an error proportional to $s_1$.\n",
    "\n",
    "Next, consider $t_2$:\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "  t_2 & = & (t_1 + p_2) (1 + \\delta_2) \\\\\n",
    "      & = & t_1 + p_2 + (t_1 + p_2)\\delta_2 \\\\\n",
    "      & = & s_1 + s_1\\delta_1 + p_2 + (s_1 + s_1\\delta_1 + p_2)\\delta_2 \\\\\n",
    "      & = & s_2 + s_1\\delta_1 + s_2\\delta_2 + s_1\\delta_1\\delta_2.\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Round-off errors should be small values, so that $x \\delta_1 \\delta_2$ may be regarded as tiny compared to $x\\delta_1$, for instance. As such, we can try to approximate $t_2$ by dropping the term containing the $\\delta_1 \\delta_2$ product, i.e.,\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "  t_2 & \\approx & s_2 + s_1\\delta_1 + s_2\\delta_2.\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Let's try one more partial sum, $t_3$, to see if a pattern emerges.\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "  t_3 &    =    & t_2 + p_3 + (t_2 + p_3)(1 + \\delta_3) \\\\\n",
    "      & \\approx & s_2 + s_1\\delta_1 + s_2\\delta_2 + p_3\n",
    "                      + (s_2 + s_1\\delta_1 + s_2\\delta_2 + p_3)\\delta_3 \\\\\n",
    "      & \\approx & s_3 + s_1\\delta_1 + s_2\\delta_2 + s_3\\delta_3.\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Continuing, we would find,\n",
    "\n",
    "$$t_{n-1} \\approx s_{n-1} + \\sum_{i=1}^{n-1} s_i \\delta_i,$$\n",
    "\n",
    "that is, the exact sum $s_{n-1}$ plus some accumulated errors.\n",
    "\n",
    "Given this partial result, let's analyze it in two ways: one to show when we should expect the algorithm to be backward stable, and another to derive a bound on the algorithm's relative error.\n",
    "\n",
    "**Analysis 1: Backward stability.** Let's rearrange the right-hand side of the above approximation:\n",
    "\n",
    "$$t_{n-1} \\approx \\sum_{i=0}^{n-1} (p_i + s_i \\delta_i).$$\n",
    "\n",
    "This rewrite shows that the computed result, $t_{n-1}$, nearly exactly corresponds to the solution of a _perturbed_ problem. If all the $s_i \\delta_i$ are \"small\" relative to the corresponding $p_i$ values, then this algorithm is backward stable.\n",
    "\n",
    "**Analysis 2: A relative error bound.** Our model of floating-point arithmetic says that each $\\delta_i$ is bounded in absolute value by machine epsilon, $\\epsilon$. Thus, we can obtain a bound on the absolute error,\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "  \\left|t_{n-1} - s_{n-1}\\right|\n",
    "    & \\approx & \\left|\\sum_{i=1}^{n-1} s_i \\delta_i\\right| \\\\\n",
    "    &   \\leq  & \\epsilon \\sum_{i=1}^{n-1} \\left|s_i\\right| \\\\\n",
    "    &   \\leq  & n \\epsilon \\left|s_{n-1}\\right|,\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "where we used the fact that $|s_i| \\leq |s_{i+1}|$ and assumed $n \\gg 1$. This final calculation also yields a _relative_ error:\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "  \\frac{\\left|t_{n-1} - s_{n-1}\\right|}\n",
    "       {\\left|s_{n-1}\\right|}\n",
    "    & \\approx & \\left|\\sum_{i=1}^{n-1} s_i \\delta_i\\right| \\\\\n",
    "    &   \\leq  & \\epsilon \\sum_{i=1}^{n-1} \\left|s_i\\right| \\\\\n",
    "    &   \\leq  & n \\epsilon.\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "In other words, the relative error accumulates linearly and is bounded by machine epsilon!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compare the output of `alg_sum()` to the relative error bound derived above for various values of $n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n = [10, 100, 1000, 10000, 100000, 1000000, 10000000]\n",
    "x = [0.1] * max (n)\n",
    "s = [1., 10., 100., 1000., 10000., 100000., 1000000.] # exact result\n",
    "t = [alg_sum (x[0:n_i]) for n_i in n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rel_err_computed = [abs (ti-si) / abs (si) for (si, ti) in zip (s, t)]\n",
    "rel_err_bound = [float (ni) * (2.**(-52)) for ni in n]\n",
    "plt.loglog (n, rel_err_computed, 'b*', n, rel_err_bound, 'r--')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise: Computing dot products.** Let $x$ and $y$ be two vectors of length $n$, and denote their dot product by $f(x, y) \\equiv x^T y$.\n",
    "\n",
    "Now suppose we store the values of $x$ and $y$ _exactly_ in two Python arrays, `x[0:n]` and `y[0:n]`. Further suppose we compute their dot product by the following program:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def alg_dot (x, y):\n",
    "    p = [xi*yi for (xi, yi) in zip (x, y)]\n",
    "    s = alg_sum (p)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Derive a relative error bound for `alg_dot()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_@YOUSE: Enter your solution here_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
